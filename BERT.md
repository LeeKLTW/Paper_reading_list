# Intuition & innovation
1. Bidirecitonal Transformer
2. Pretrain & fine-tune (Note the difference between feature.)
3. Pretrain task (1/2) MLM for word-level

This task is importance, in traditional LM, if you assume it is bidiretional, you inheritantly "make the word see itself".

3.1. Mismath problem fixing.
3.2. Might be slower, but worth it.
4. Pretrain task (2/2) Next sentence prediction.
Binary clf.

# Compare
v.s. GPT, ELMo 

checkout fig1 & 3.6

# Evaluation

11 NLP task

# Detail
1. BERT-Base version & BERT Large version
2. gelu not relu.
