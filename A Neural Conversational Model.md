Our preliminary results suggest that, despite
optimizing the wrong objective function,
the model is able to converse well. It is able
extract knowledge from both a domain specific
dataset, and from a large, noisy, and general domain
dataset of movie subtitles. On a domainspecific
IT helpdesk dataset, the model can find
a solution to a technical problem via conversations.
On a noisy open-domain movie transcript
dataset, the model can perform simple forms of
common sense reasoning.

...

Personal note:
In 'IT Helpdesk Troubleshooting dataset' experiment:

400 words long

300M tokens in train

3M tokens in test

most 20k common words index

1024 memory cells

SGD

this model perplexity 8 vs n-gram 18



In Open Subtitles 
1. Basic
2. Simple Q&A
3. General knowledge
4. Philosophical
5. Morality
6. Opinions
7. Job & personality
